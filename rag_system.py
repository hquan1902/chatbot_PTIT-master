import glob
import os
import shutil
import hashlib
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# --- Load OpenAI API key ---
load_dotenv()
if "OPENAI_API_KEY" not in os.environ:
    print("Lá»—i: OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p. Vui lÃ²ng táº¡o file .env vÃ  thÃªm key vÃ o.")
    exit()

# --- Cáº¥u hÃ¬nh ---
EMBEDDING_MODEL = "text-embedding-3-small"
CHROMA_DB_PATH = "./knowledge_base_ptit"
OLD_DOCS_DIR = "./old_docs"
NEW_DOCS_DIR = "./new_docs"

# === TIá»†N ÃCH HASH Äá»‚ PHÃT HIá»†N TRÃ™NG Láº¶P ===
def compute_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

# === LOAD FILE ÄA Äá»ŠNH Dáº NG ===
def load_text_from_file(file_path: str):
    ext = os.path.splitext(file_path)[1].lower()
    try:
        if ext == ".txt":
            loader = TextLoader(file_path, encoding="utf-8")
        elif ext == ".pdf":
            loader = PyPDFLoader(file_path)
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
        elif ext == ".csv":
            # Chá»‰ dÃ¹ng náº¿u CSV chá»©a vÄƒn báº£n (khÃ´ng pháº£i dá»¯ liá»‡u báº£ng)
            loader = CSVLoader(file_path)
        else:
            print(f"âŒ Bá» qua file khÃ´ng há»— trá»£: {file_path}")
            return []
        return loader.load()
    except Exception as e:
        print(f"âš ï¸ Lá»—i Ä‘á»c file {file_path}: {e}")
        return []

# === Xá»¬ LÃ VÄ‚N Báº¢N ===
def load_and_process_documents(docs_dir: str):
    """Äá»c & chia nhá» tÃ i liá»‡u"""
    documents = []
    for filename in os.listdir(docs_dir):
        path = os.path.join(docs_dir, filename)
        if os.path.isfile(path):
            docs = load_text_from_file(path)
            for d in docs:
                d.metadata["file_name"] = filename
            documents.extend(docs)

    if not documents:
        return [], []

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    return chunks, [os.path.join(docs_dir, f) for f in os.listdir(docs_dir)]

# === KHá»I Táº O VECTOR STORE ===
def initialize_vector_store(db_path: str, embedding_model: str, docs_dir: str):
    embeddings = OpenAIEmbeddings(model=embedding_model)

    if os.path.exists(db_path):
        print(f"Äang load Knowledge Base tá»« '{db_path}'...")
        vector_store = Chroma(persist_directory=db_path, embedding_function=embeddings)
    else:
        print(f"Táº¡o má»›i Knowledge Base tá»« '{docs_dir}'...")
        chunks, _ = load_and_process_documents(docs_dir)
        vector_store = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=db_path)
        print(f"âœ… Knowledge Base Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  lÆ°u vÃ o '{db_path}'.")

    return vector_store


# === Cáº¬P NHáº¬T DATABASE (PHIÃŠN Báº¢N XÃ“A FILE Lá»–I/TRÃ™NG Láº¶P) ===
def check_and_update_database(vector_store: Chroma, new_docs_dir: str, old_docs_dir: str):
    """
    Cáº­p nháº­t DB.
    - File má»›i, há»£p lá»‡ -> Chuyá»ƒn vÃ o old_docs.
    - File lá»—i hoáº·c ná»™i dung trÃ¹ng láº·p -> XÃ³a vÄ©nh viá»…n.
    """
    if not os.path.exists(new_docs_dir) or not os.listdir(new_docs_dir):
        print("ğŸ“‚ KhÃ´ng cÃ³ file má»›i trong 'new_docs'.")
        return

    new_chunks, processed_files = load_and_process_documents(new_docs_dir)
    
    # --- Xá»¬ LÃ TRÆ¯á»œNG Há»¢P FILE Bá»Š Lá»–I, KHÃ”NG Äá»ŒC ÄÆ¯á»¢C ---
    if not new_chunks:
        print("âš™ï¸ KhÃ´ng cÃ³ ná»™i dung há»£p lá»‡ hoáº·c khÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c file.")
        for f in processed_files:
            try:
                if os.path.exists(f):
                    os.remove(f)
                    print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a file lá»—i: {os.path.basename(f)}")
            except Exception as e:
                print(f"âš ï¸ Lá»—i khi xÃ³a file {os.path.basename(f)}: {e}")
        return

    # --- Xá»¬ LÃ KIá»‚M TRA TRÃ™NG Láº¶P Ná»˜I DUNG ---
    existing_hashes = set()
    try:
        existing_docs = vector_store.get(include=["metadatas"])
        for meta in existing_docs["metadatas"]:
            if "hash" in meta:
                existing_hashes.add(meta["hash"])
    except Exception:
        pass

    unique_chunks = []
    for chunk in new_chunks:
        h = compute_hash(chunk.page_content)
        if h not in existing_hashes:
            chunk.metadata["hash"] = h
            unique_chunks.append(chunk)
            existing_hashes.add(h)

    # --- QUYáº¾T Äá»ŠNH XÃ“A HAY DI CHUYá»‚N Dá»°A TRÃŠN Káº¾T QUáº¢ ---
    
    # TrÆ°á»ng há»£p 1: CÃ³ ná»™i dung má»›i, há»£p lá»‡
    if unique_chunks:
        print(f"ğŸ§  Äang thÃªm {len(unique_chunks)} Ä‘oáº¡n má»›i...")
        vector_store.add_documents(unique_chunks)
        
        # Di chuyá»ƒn file gá»‘c vÃ o old_docs Ä‘á»ƒ lÆ°u trá»¯
        os.makedirs(old_docs_dir, exist_ok=True)
        for f in processed_files:
            if os.path.exists(f):
                shutil.move(f, os.path.join(old_docs_dir, os.path.basename(f)))
        
        print(f"ğŸ‰ ÄÃ£ thÃªm tri thá»©c má»›i vÃ  di chuyá»ƒn {len(processed_files)} file gá»‘c sang 'old_docs'.")

    # TrÆ°á»ng há»£p 2: ToÃ n bá»™ ná»™i dung Ä‘á»u Ä‘Ã£ tá»“n táº¡i (trÃ¹ng láº·p)
    else:
        print("âœ… ToÃ n bá»™ ná»™i dung trong file Ä‘Ã£ tá»“n táº¡i trong tri thá»©c.")
        for f in processed_files:
            try:
                if os.path.exists(f):
                    os.remove(f)
                    print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a file trÃ¹ng láº·p: {os.path.basename(f)}")
            except Exception as e:
                print(f"âš ï¸ Lá»—i khi xÃ³a file {os.path.basename(f)}: {e}")


# === HÃ€M Gá»ŒI Tá»ª FLASK ===
def update_knowledge_base_auto():
    """HÃ m tá»± Ä‘á»™ng cáº­p nháº­t khi upload tá»« Flask"""
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    os.makedirs(NEW_DOCS_DIR, exist_ok=True)
    db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
    check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)

# === MAIN CHáº Y Äá»˜C Láº¬P ===
if __name__ == "__main__":
    print("=== Báº®T Äáº¦U Cáº¬P NHáº¬T CÆ  Sá» TRI THá»¨C ===\n")
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    os.makedirs(NEW_DOCS_DIR, exist_ok=True)
    db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
    check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)
    print("\n=== HOÃ€N Táº¤T ===")
