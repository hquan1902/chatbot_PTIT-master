# import glob
# import os
# import shutil
# from dotenv import load_dotenv
# from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_chroma import Chroma
# from langchain_openai import OpenAIEmbeddings

# load_dotenv()
# if "OPENAI_API_KEY" not in os.environ:
#     print("Lá»—i: OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p. Vui lÃ²ng táº¡o file .env vÃ  thÃªm key vÃ o.")
#     exit()

# EMBEDDING_MODEL = "text-embedding-3-small"
# CHROMA_DB_PATH = "./knowledge_base_ptit"
# OLD_DOCS_DIR = "./old_docs"
# NEW_DOCS_DIR = "./new_docs"


# def load_and_process_documents(docs_dir: str):
#     """Load vÃ  xá»­ lÃ½ tÃ i liá»‡u tá»« thÆ° má»¥c"""
#     try:
#         loader = DirectoryLoader(
#             docs_dir,
#             glob="**/*",
#             loader_cls=UnstructuredFileLoader,
#             show_progress=True,
#             use_multithreading=True
#         )
#         documents = loader.load()
#         if not documents:
#             return [], []

#         for doc in documents:
#             if "source" in doc.metadata:
#                 doc.metadata["file_name"] = os.path.basename(doc.metadata["source"])

#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=1000,
#             chunk_overlap=200
#         )
#         chunks = text_splitter.split_documents(documents)
#         source_files = list(set([doc.metadata.get("source") for doc in documents]))
#         return chunks, source_files
#     except Exception as e:
#         print(f"Lá»—i khi load tÃ i liá»‡u: {e}")
#         return [], []


# def initialize_vector_store(db_path: str, embedding_model: str, docs_dir: str):
#     """Khá»Ÿi táº¡o hoáº·c load Vector Store"""
#     try:
#         embeddings = OpenAIEmbeddings(model=embedding_model)

#         if os.path.exists(db_path):
#             print(f"Äang load Knowledge Base tá»« '{db_path}'...")
#             vector_store = Chroma(
#                 persist_directory=db_path,
#                 embedding_function=embeddings
#             )
#         else:
#             print(f"Knowledge Base chÆ°a tá»“n táº¡i. Äang táº¡o má»›i tá»« '{docs_dir}'...")
#             chunks, _ = load_and_process_documents(docs_dir)

#             if not chunks:
#                 print("KhÃ´ng cÃ³ tÃ i liá»‡u ban Ä‘áº§u, táº¡o má»™t Knowledge Base rá»—ng.")
#                 vector_store = Chroma(
#                     embedding_function=embeddings,
#                     persist_directory=db_path
#                 )
#             else:
#                 print(f"Äang embedding {len(chunks)} chunks...")
#                 vector_store = Chroma.from_documents(
#                     chunks,
#                     embedding=embeddings,
#                     persist_directory=db_path
#                 )

#             print(f"ÄÃ£ táº¡o vÃ  lÆ°u Knowledge Base vÃ o '{db_path}'.")

#         return vector_store
#     except Exception as e:
#         print(f"Lá»—i khi khá»Ÿi táº¡o Vector Store: {e}")
#         exit(1)


# def check_and_update_database(vector_store: Chroma, new_docs_dir: str, old_docs_dir: str):
#     """Kiá»ƒm tra vÃ  cáº­p nháº­t database vá»›i tÃ i liá»‡u má»›i"""
#     try:
#         if not os.path.exists(new_docs_dir) or not os.listdir(new_docs_dir):
#             print("ThÆ° má»¥c 'new_docs' rá»—ng, khÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ cáº­p nháº­t.")
#             return

#         new_chunks, processed_files = load_and_process_documents(new_docs_dir)
#         if not new_chunks:
#             print("KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u má»›i há»£p lá»‡.")
#             return

#         # Láº¥y tÃªn file tá»« metadata
#         new_file_names = set()
#         for chunk in new_chunks:
#             if "file_name" in chunk.metadata:
#                 new_file_names.add(chunk.metadata["file_name"])

#         old_files = [
#             os.path.basename(f)
#             for f in glob.glob(os.path.join(old_docs_dir, "**", "*.*"), recursive=True)
#         ]

#         existing = [f for f in old_files if f in new_file_names]

#         if existing:
#             print(f"ÄÃ£ phÃ¡t hiá»‡n {len(existing)} file cÅ©, Ä‘ang cáº­p nháº­t...")
#             for chunk in new_chunks:
#                 if "file_name" in chunk.metadata and chunk.metadata["file_name"] in existing:
#                     vector_store.delete(where={"file_name": chunk.metadata["file_name"]})
#             print(f"ÄÃ£ xÃ³a {len(existing)} file cÅ© khá»i Knowledge Base.")

#         print(f"Äang thÃªm {len(new_chunks)} chunks má»›i vÃ o Knowledge Base...")
#         vector_store.add_documents(new_chunks)

#         # Di chuyá»ƒn file tá»« new_docs sang old_docs
#         for file_path in processed_files:
#             file_name = os.path.basename(file_path)
#             destination_path = os.path.join(old_docs_dir, file_name)
#             if os.path.exists(destination_path):
#                 os.remove(destination_path)
#             shutil.move(file_path, destination_path)

#         print(f"ÄÃ£ di chuyá»ƒn {len(processed_files)} file sang 'old_docs'.")
#         print("--- HoÃ n táº¥t quy trÃ¬nh cáº­p nháº­t ---")

#     except Exception as e:
#         print(f"Lá»—i khi cáº­p nháº­t database: {e}")


# # âœ… HÃ€M THÃŠM Má»šI CHO FLASK
# def update_knowledge_base_auto():
#     """HÃ m tá»± Ä‘á»™ng cáº­p nháº­t Knowledge Base mÃ  khÃ´ng cáº§n nháº­p tay (dÃ¹ng cho Flask)."""
#     os.makedirs(OLD_DOCS_DIR, exist_ok=True)
#     os.makedirs(NEW_DOCS_DIR, exist_ok=True)

#     db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
#     try:
#         check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)
#     except Exception as e:
#         print(f"Lá»—i khi tá»± Ä‘á»™ng cáº­p nháº­t: {e}")


# if __name__ == "__main__":
#     print("=== Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh xÃ¢y dá»±ng/cáº­p nháº­t Knowledge Base ===\n")
#     os.makedirs(OLD_DOCS_DIR, exist_ok=True)
#     os.makedirs(NEW_DOCS_DIR, exist_ok=True)
#     db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
#     check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)
#     print("\n=== HoÃ n táº¥t ===")





import glob
import os
import shutil
import hashlib
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# --- Load OpenAI API key ---
load_dotenv()
if "OPENAI_API_KEY" not in os.environ:
    print("Lá»—i: OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p. Vui lÃ²ng táº¡o file .env vÃ  thÃªm key vÃ o.")
    exit()

# --- Cáº¥u hÃ¬nh ---
EMBEDDING_MODEL = "text-embedding-3-small"
CHROMA_DB_PATH = "./knowledge_base_ptit"
OLD_DOCS_DIR = "./old_docs"
NEW_DOCS_DIR = "./new_docs"

# === TIá»†N ÃCH HASH Äá»‚ PHÃT HIá»†N TRÃ™NG Láº¶P ===
def compute_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

# === LOAD FILE ÄA Äá»ŠNH Dáº NG ===
def load_text_from_file(file_path: str):
    ext = os.path.splitext(file_path)[1].lower()
    try:
        if ext == ".txt":
            loader = TextLoader(file_path, encoding="utf-8")
        elif ext == ".pdf":
            loader = PyPDFLoader(file_path)
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
        elif ext == ".csv":
            # Chá»‰ dÃ¹ng náº¿u CSV chá»©a vÄƒn báº£n (khÃ´ng pháº£i dá»¯ liá»‡u báº£ng)
            loader = CSVLoader(file_path)
        else:
            print(f"âŒ Bá» qua file khÃ´ng há»— trá»£: {file_path}")
            return []
        return loader.load()
    except Exception as e:
        print(f"âš ï¸ Lá»—i Ä‘á»c file {file_path}: {e}")
        return []

# === Xá»¬ LÃ VÄ‚N Báº¢N ===
def load_and_process_documents(docs_dir: str):
    """Äá»c & chia nhá» tÃ i liá»‡u"""
    documents = []
    for filename in os.listdir(docs_dir):
        path = os.path.join(docs_dir, filename)
        if os.path.isfile(path):
            docs = load_text_from_file(path)
            for d in docs:
                d.metadata["file_name"] = filename
            documents.extend(docs)

    if not documents:
        return [], []

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    return chunks, [os.path.join(docs_dir, f) for f in os.listdir(docs_dir)]

# === KHá»I Táº O VECTOR STORE ===
def initialize_vector_store(db_path: str, embedding_model: str, docs_dir: str):
    embeddings = OpenAIEmbeddings(model=embedding_model)

    if os.path.exists(db_path):
        print(f"Äang load Knowledge Base tá»« '{db_path}'...")
        vector_store = Chroma(persist_directory=db_path, embedding_function=embeddings)
    else:
        print(f"Táº¡o má»›i Knowledge Base tá»« '{docs_dir}'...")
        chunks, _ = load_and_process_documents(docs_dir)
        vector_store = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=db_path)
        print(f"âœ… Knowledge Base Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  lÆ°u vÃ o '{db_path}'.")

    return vector_store

# === Cáº¬P NHáº¬T DATABASE ===
def check_and_update_database(vector_store: Chroma, new_docs_dir: str, old_docs_dir: str):
    """Cáº­p nháº­t DB khi cÃ³ file má»›i (loáº¡i trÃ¹ng ná»™i dung theo hash)"""
    if not os.path.exists(new_docs_dir) or not os.listdir(new_docs_dir):
        print("ğŸ“‚ KhÃ´ng cÃ³ file má»›i trong 'new_docs'.")
        return

    new_chunks, processed_files = load_and_process_documents(new_docs_dir)
    if not new_chunks:
        print("âš™ï¸ KhÃ´ng cÃ³ tÃ i liá»‡u há»£p lá»‡.")
        return

    # Táº¡o set hash cá»§a cÃ¡c Ä‘oáº¡n Ä‘Ã£ cÃ³
    existing_hashes = set()
    try:
        existing_docs = vector_store.get(include=["metadatas"])
        for meta in existing_docs["metadatas"]:
            if "hash" in meta:
                existing_hashes.add(meta["hash"])
    except Exception:
        pass

    # Gáº¯n hash má»›i vÃ  lá»c trÃ¹ng
    unique_chunks = []
    for chunk in new_chunks:
        h = compute_hash(chunk.page_content)
        if h not in existing_hashes:
            chunk.metadata["hash"] = h
            unique_chunks.append(chunk)
            existing_hashes.add(h)

    if not unique_chunks:
        print("âœ… KhÃ´ng cÃ³ Ä‘oáº¡n má»›i (toÃ n bá»™ ná»™i dung Ä‘Ã£ tá»“n táº¡i).")
        return

    print(f"ğŸ§  Äang thÃªm {len(unique_chunks)} Ä‘oáº¡n má»›i...")
    vector_store.add_documents(unique_chunks)
    vector_store.persist()

    os.makedirs(old_docs_dir, exist_ok=True)
    for f in processed_files:
        shutil.move(f, os.path.join(old_docs_dir, os.path.basename(f)))

    print(f"ğŸ‰ ÄÃ£ thÃªm {len(unique_chunks)} Ä‘oáº¡n má»›i vÃ  di chuyá»ƒn file sang 'old_docs'.")

# === HÃ€M Gá»ŒI Tá»ª FLASK ===
def update_knowledge_base_auto():
    """HÃ m tá»± Ä‘á»™ng cáº­p nháº­t khi upload tá»« Flask"""
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    os.makedirs(NEW_DOCS_DIR, exist_ok=True)
    db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
    check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)

# === MAIN CHáº Y Äá»˜C Láº¬P ===
if __name__ == "__main__":
    print("=== Báº®T Äáº¦U Cáº¬P NHáº¬T CÆ  Sá» TRI THá»¨C ===\n")
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    os.makedirs(NEW_DOCS_DIR, exist_ok=True)
    db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)
    check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)
    print("\n=== HOÃ€N Táº¤T ===")
